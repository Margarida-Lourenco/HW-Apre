\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{multirow}
\usepackage{xcolor}

\setlength{\droptitle}{-6em}

\begin{document}

\begin{center}
Aprendizagem 2023\\
Homework I --- Group 003\\
(ist1107028, ist1107137)\vskip 1cm
\end{center}

\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep]
\item Complete the given decision tree using Shannon entropy ($\log_2$) and considering that:
    (i) a minimum of 4 observations is required to split an internal node, and (ii) decisions by ascending alphabetic should be placed in case of ties.

\begin{equation*}
    \begin{split}
        H(y_{\text{out}}|y_1 > 0.3) = - P(y_{\text{out}} = A|y_1 > 0.3)\log_2(P(y_{\text{out}} = A|y_1 > 0.3)) \\
        H(y_{\text{out}}|y_1 > 0.3) = - P(y_{\text{out}} = B|y_1 > 0.3)\log_2(P(y_{\text{out}} = B|y_1 > 0.3)) \\
        H(y_{\text{out}}|y_1 > 0.3) = - P(y_{\text{out}} = C|y_1 > 0.3)\log_2(P(y_{\text{out}} = C|y_1 > 0.3)) \\
    \end{split}
\end{equation*}

\begin{equation*}
    H(y_{\text{out}}|y_1 > 0.3) = - \left( \frac{3}{6} \log_2 \left( \frac{3}{6} \right) + \frac{1}{6} \log_2 \left( \frac{1}{6} \right) + \frac{2}{6} \log_2 \left( \frac{2}{6} \right) \right) \approx  1.4591
\end{equation*}

\begin{equation}
    \begin{split}
    H(y_{\text{out}} | y_1 > 0.3, y_x) = P(y_x = 0 | y_1 > 0.3) H(y_{\text{out}} | y_1 > 0.3, y_x = 0) \\
    + P(y_x = 1 | y_1 > 0.3) H(y_{\text{out}} | y_1 > 0.3, y_x = 1) \\
    + P(y_x = 2 | y_1 > 0.3) H(y_{\text{out}} | y_1 > 0.3, y_x = 2)
    \end{split}
\end{equation}

\begin{equation}
    IG(y_{\text{out}} | y_1 > 0.3, y_x) = H(y_{\text{out}} | y_1 > 0.3) - H(y_{\text{out}} | y_1 > 0.3, y_x)
\end{equation}

\vspace{0.5cm}
\quad\fbox{x=2:}

\begin{equation*}
    P(y_2 = 0 | y_1 > 0.3) = \frac{3}{6} \quad
    P(y_2 = 1 | y_1 > 0.3) = \frac{3}{6} \quad
    P(y_2 = 2 | y_1 > 0.3) = \frac{0}{6} \quad
\end{equation*}

\begin{equation*}
    \begin{aligned}
        H(y_{\text{out}} | y_1 > 0.3, y_2 = 0) &= - \left( \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{0}{3} \log_2 \left( \frac{0}{3} \right) + \frac{2}{3} \log_2 \left( \frac{2}{3} \right) \right) \approx 0.9183
        \\
        H(y_{\text{out}} | y_1 > 0.3, y_2 = 1) &= - \left( \frac{2}{3} \log_2 \left( \frac{2}{3} \right) + \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{0}{3} \log_2 \left( \frac{0}{3} \right) \right) \approx 0.9183
        \\
        H(y_{\text{out}} | y_1 > 0.3, y_2 = 2) &= - \left( \frac{0}{0} \log_2 \left( \frac{0}{0} \right) + \frac{0}{0} \log_2 \left( \frac{0}{0} \right) + \frac{0}{0} \log_2 \left( \frac{0}{0} \right) \right) = 0
        \\
    \end{aligned}
\end{equation*}

\textcolor{blue}{ou então colocar "Como não existem B's nas condições avaliadas, a entropia é zero"}

\begin{equation*}
    H(y_{\text{out}} | y_1 > 0.3, y_2) = \frac{3}{6} \times 0.91830 + \frac{3}{6} \times 0.9183 + \frac{0}{6} \times 0 \approx 0.9183
    \\
\end{equation*}

\begin{equation*}
    IG(y_{\text{out}} | y_1 > 0.3, y_2) = 1.4591 - 0.9183 = 0.5408
\end{equation*}

\newpage

\fbox{x=3:}

\begin{equation*}
    P(y_3 = 0|y_1 > 0.3) = \frac{2}{6} \quad
    P(y_3 = 1|y_1 > 0.3) = \frac{3}{6} \quad
    P(y_3 = 2|y_1 > 0.3) = \frac{1}{6} \quad
\end{equation*}

\begin{equation*}
    \begin{aligned}
        E(y_{\text{out}}|y_1 > 0.3, y_3 = 0) = - \left( \frac{2}{2} \log_2 \left( \frac{2}{2} \right) + \frac{0}{2} \log_2 \left( \frac{0}{2} \right) + \frac{0}{2} \log_2 \left( \frac{0}{2} \right) \right) &= 0 
        \\
        E(y_{\text{out}}|y_1 > 0.3, y_3 = 1) = - \left( \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{0}{3} \log_2 \left( \frac{0}{3} \right) + \frac{2}{3} \log_2 \left( \frac{2}{3} \right) \right) &\approx 0.91830
        \\
        E(y_{\text{out}}|y_1 > 0.3, y_3 = 2) = - \left( \frac{0}{1} \log_2 \left( \frac{0}{1} \right) + \frac{1}{1} \log_2 \left( \frac{1}{1} \right) + \frac{0}{1} \log_2 \left( \frac{0}{1} \right) \right) &= 0
    \end{aligned}
\end{equation*}

\begin{equation*}
    E(y_{\text{out}}|y_1 > 0.3, y_3) = \frac{2}{6} \times  0 + \frac{3}{6} \times 0.9183 + \frac{1}{6} \times 0 \approx 0.4592
\end{equation*}

\begin{equation*}
    IG(y_{\text{out}} | y_1 > 0.3, y_3) = 1.4591 - 0.4592 = \textbf{0.9999}
\end{equation*}

\quad\fbox{x=4:}

\begin{equation*}
    P(y_4 = 0|y_1 > 0.3) = \frac{3}{6} \quad
    P(y_4 = 1|y_1 > 0.3) = \frac{3}{6} \quad
    P(y_4 = 2|y_1 > 0.3) = \frac{0}{6} \quad
\end{equation*}

\begin{equation*}
    \begin{aligned}
        E(y_{\text{out}}|y_1 > 0.3, y_4 = 0) = - \left( \frac{2}{3} \log_2 \left( \frac{2}{3} \right) + \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{0}{3} \log_2 \left( \frac{0}{3} \right) \right) &\approx 0.9183 
        \\
        E(y_{\text{out}}|y_1 > 0.3, y_4 = 1) = - \left( \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{0}{3} \log_2 \left( \frac{0}{3} \right) + \frac{2}{3} \log_2 \left( \frac{2}{3} \right) \right) &\approx 0.9183
        \\
    \end{aligned}
\end{equation*}

\textcolor{blue}{mesma situação de $x=2$}

\begin{equation*}
    E(y_{\text{out}}|y_1 > 0.3, y_4) = \frac{3}{6} \times 0.9183 + \frac{3}{6} \times 0.9183 + \frac{0}{6} \times 0 \approx 0.9183
\end{equation*}

\begin{equation*}
    IG(y_{\text{out}} | y_1 > 0.3, y_4) = 1.4591 - 0.9183 = 0.5408
\end{equation*}

\vspace{0.5cm}

\hspace*{2em}After calculating the information gains for each attribute, we can observe that the attribute $y_3$ has the highest value of 0.9999. 
Accordingly, we chose it as the next node. Since there are at least four observations with $y_1 > 0.3$, we split the new node. 

\hspace*{2em}If we fix $y_1 > 0.3$ with $y_3 = 0$, $y_3 = 1$ and $y_3 = 2$, we will obtain 2, 3 and 1 observations, respectively. This gives us three new leaves for the branch in question. The node corresponding to $y_3 = 0$ will be class A, the one with $y_3 = 1$ will be class C, and the remaining, $y_3 = 2$ will be from class B, since these classes are the ones that appear most frequently for the respective conditions in the data set.

%\begin{figure}[H]
%          \centering
%          \includegraphics[width=12cm]{./assets/decision_tree_ex1_PartI.png}
%          \caption{Decision Tree for exercise I-1}
%          \label{fig:decision_tree}
%        \end{figure}

\newpage

\item Draw the training confusion matrix for the learnt decision tree.
\begin{center}
    \begin{tabular}{cccccccccccccc}
        \multicolumn{2}{c}{}  $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ & $x_8$ & $x_9$ & $x_{10}$ & $x_{11}$ & $x_{12}$ \\
        \multirow{1}{*}{real}      & =    [ A & B & B & C & C & A & A & A & B & B & C & C  ] \\
        \multirow{1}{*}{predicted} & =   [ A & B & \textcolor{red}{\textbf{C}} & C & C & A & A & A & \textcolor{red}{\textbf{A}} & B & C & C  ]
  \end{tabular}
\end{center}
    
\item For more details on putting math into {\LaTeX} documents you can see 

\item We you get to the next problem, you can end the enumerate for the parts of the previous problem and then add another item.
\end{enumerate}
\begin{enumerate}
    \item Use a nested enumerate environment to label the parts of the next problem.
    \item For a quick and broad overview of how to create documents in {\LaTeX} see 
\end{enumerate}

\newpage

\large{\textbf{Part II}: Programming}\normalsize

\begin{enumerate}[leftmargin=\labelsep,resume]
\item Solution to the programming questions here.
\end{enumerate}

\vskip 1cm
\textbf{End note}: do not forget to also submit your Jupyter notebook

\end{document}
